from airflow import DAG
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator
from datetime import datetime

default_args = {"owner": "airflow", "retries": 0}

with DAG(
    dag_id="spark_daily_incremental_load_tbl_T_BACK_ADVANCE_WITHDRAW",
    start_date=datetime(2025, 11, 18),
    schedule_interval="0 1 * * *",
    catchup=False,
    tags=["spark", "delta", "hive", "daily"]
) as dag:

    daily_incremental_load = SparkSubmitOperator(
        task_id="daily_incremental_load_tbl_T_BACK_ADVANCE_WITHDRAW",
        application="s3a://asset/spark-jobs/daily_incremental_load_tbl_T_BACK_ADVANCE_WITHDRAW.py",
        deploy_mode="cluster",
        name="spark-daily-incremental-load-tbl_T_BACK_ADVANCE_WITHDRAW",
        conn_id="spark_k8s",
        conf={
            "spark.kubernetes.namespace": "compute",
            "spark.kubernetes.container.image": "ghostwood/mbs-spark:1.0.1-oracle",
            "spark.kubernetes.authenticate.driver.serviceAccountName": "spark",
            "spark.sql.extensions": "io.delta.sql.DeltaSparkSessionExtension",
            "spark.sql.catalog.spark_catalog": "org.apache.spark.sql.delta.catalog.DeltaCatalog",
            "spark.sql.catalogImplementation": "hive",
            "spark.hadoop.hive.metastore.uris": "thrift://hive-metastore.metastore.svc.cluster.local:9083",
            "spark.sql.warehouse.dir": "s3a://warehouse/",
            "spark.hadoop.fs.s3a.endpoint": "http://minio.storage.svc.cluster.local:9000",
            "spark.hadoop.fs.s3a.access.key": "minioadmin",
            "spark.hadoop.fs.s3a.secret.key": "minio@demo!",
            "spark.hadoop.fs.s3a.path.style.access": "true",
            "spark.hadoop.fs.s3a.impl": "org.apache.hadoop.fs.s3a.S3AFileSystem",
            "spark.sql.sources.partitionOverwriteMode": "dynamic",
            "conf spark.eventLog.dir": "s3a://spark-logs/events",
            "spark.driver.extraJavaOptions": "-Divy.cache.dir=/tmp -Divy.home=/tmp"
        },
        verbose=True
    )

    daily_incremental_load